---
layout: page
---
<head>
<!--    for publication     -->
    <style type="text/css">
        table.row td{
            background-color: white !important;
            border: 0;
            vertical-align: top;
        }
        li{
            list-style-type: none;
        }
        a.paper_link
        {
            display: block; 
            text-decoration: none;
            font-weight: bold;
        }
        
        @media only screen and (max-device-width: 767px) { 
            .imgtd {
                display: none;
            }
            img.smallthumbnail {
                display: block;
                margin-top: 3em;
            }
        }
        
        
        @media only screen and (min-device-width: 768px) { 
            img.smallthumbnail {
                display: none;
            }
        }
        
        .imgtd {
            width: 35%;
        }
        
        img.thumbnail
        {
            margin-top: 15px;
            padding-left: 10px;
            border-radius: 10px;
        }
        
        img.inline {
            display: inline;
            margin: 0;
        }
        
       .title {
            font-size: large;
            font-family: sans-serif;
        }
        .description{
            font-size: medium;
        }
        .detail{
            font-size: small;
        }
    </style>
</head>

<!-- PUBLICATIONS -->
<!-- <h2><strong>Publications</strong></h2> -->
<h2> Intelligent Text Entry Application </h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <img class="smallthumbnail" src="/assets/img/research/TTC.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/TTC.png" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="https://faculty.washington.edu/wobbrock/pubs/uist-19.02.pdf" target="_blank"> Type, Then Correct: Intelligent Text Correction Techniques for Mobile Text Entry Using Neural Networks </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, He Wen, Jacob O. Wobbrock
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2019
                    <br>
                    <font color="green" class="detail">Instead of normal touch+cursor based correction process, why cannot we rethink of the correction interaction? In this paper, we present three novel interactions that allow the user to type the correction first, then apply it to the error place. Furthermore, we applied deep learning technology to enable automatic error detection for the interaction. </font>
            </span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/ATK.jpg" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/ATK.jpg" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="http://pi.cs.tsinghua.edu.cn/lab/papers/p539-yi.pdf" target="_blank"> ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data </a></span>
                <span class="description">Xin Yi, Chun Yu, <strong>Mingrui Zhang</strong>, Sida Gao
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2015
                    <br>
                    <font color="green" class="detail">A novel air-typing method, Leapmotion tracking fingers, improved Bayes prediction model with application developed. Users reached the speed of 29.2 WPM on average.</font>
            </span></li>
        </div></td></tr>
    </table>

<h2> Text Entry Evaluation </h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <img class="smallthumbnail" src="/assets/img/research/Tseq.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Tseq.png" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="https://faculty.washington.edu/wobbrock/pubs/uist-19.01.pdf" target="_blank">Beyond the Input Stream: Making Text Entry Evaluations More Flexible with Transcription Sequences </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Jacob O. Wobbrock
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2019
                    <br>
                    <font color="green" class="detail"> In this work, we present a new underlying model that supersedes the input stream model for general-purpose method-independent character-level text entry evaluation. Specifically, we present an approach that replaces the input stream with transcription sequences, or “T-sequences” for short. In brief, T-sequences are snapshots of the entire transcribed string after each text-changing action is taken by the user. Every pair of successive snapshots are then analyzed to compute character-level text entry metrics. </font> 
            </span></li>
        </div></td></tr>
    
        <img class="smallthumbnail" src="/assets/img/research/Throughput.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Throughput.png" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="http://faculty.washington.edu/wobbrock/pubs/chi-19.03.pdf" target="_blank">Text Entry Throughput: Towards Unifying Speed and Accuracy in a Single Performance Metric </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Shumin Zhai, Jacob O. Wobbrock
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2019 <a href="/posts/2019/04/28/Throughput1/" target="_blank">Related Blog Post</a>
                    <br>
                    <font color="green" class="detail">We define the text entry Throughput as a performance metric combining the speed and accuracy. Throughput is derived from the transmission ratio in the information theory. Unlike other metrics, throughput is less affected by speed-accuracy tradeoffs, thus it enables cross-device, cross-publication comparison. </font> 
            </span></li>
        </div></td></tr>
    </table>

<h2> Voice User Interface </h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <img class="smallthumbnail" src="/assets/img/research/alexa.jpg" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/alexa.jpg" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="http://faculty.washington.edu/alexisr/commBreakdowns.pdf" target="_blank">Communication Breakdowns Between Families and Alexa </a></span>
                <span class="description">Erin Beneteau, Olivia K. Richards, <strong>Mingrui Zhang</strong>, Julie A. Kientz, Jason Yip, Alexis Hiniker
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2019
                    <br>
                    <font color="green" class="detail"> We investigated different types of communication breakdowns and the repairing strategies between the conversation of family members and Alexa. Our findings indicates that improving technology’s ability to identify the communication partners and to provide specific clarification responses will ultimately improve the conversational interaction experience.</font>
            </span></li>
        </div></td></tr>
    </table>

<h2> MISC </h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <img class="smallthumbnail" src="/assets/img/research/AAS.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/AAS.png" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="https://makeabilitylab.cs.washington.edu/media/publications/Hiniker_AnchoredAudioSamplingASeamlessMethodForExploringChildrenSThoughtsDuringDeploymentStudies_2019.pdf" target="_blank">Anchored Audio Sampling: A Seamless Method for Exploring Children’s Thoughts During Deployment Studies </a></span>
                <span class="description">Alexis Hiniker, Jon E. Froehlich, <strong>Mingrui Zhang</strong>, Erin Beneteau
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2019 <img class="inline" src="/assets/img/research/bestpaper.jpg"><i>Best Paper Award</i>
                    <br>
                    <font color="green" class="detail">We present Anchored Audio Smapling (AAS) method for collecting remote data of qualitative audio samples during field development with young children. The anchor event triggers the recording, and a sliding window surrounding this anchor captures both antecedent and ensuing recording. <a href="https://github.com/uelab/KidsRecorder" target="_blank">Our AAS Library for Android</a></font>
            </span></li>
        </div></td></tr>
    </table>